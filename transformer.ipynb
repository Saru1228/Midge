{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b3c46f",
   "metadata": {},
   "source": [
    "# Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71075e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6915690",
   "metadata": {},
   "source": [
    "# Function section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cc4e4",
   "metadata": {},
   "source": [
    "### Step 1. 数据准备（Data Pipeline）\n",
    "功能\n",
    "\n",
    "将文本变成 token → id → 可输入 Transformer 的 batch。\n",
    "\n",
    "输入\n",
    "\n",
    "原始文本（字符串 list）\n",
    "\n",
    "输出\n",
    "\n",
    "input_ids（整数矩阵）\n",
    "\n",
    "attention_mask（0/1）\n",
    "\n",
    "target_ids（decoder 用）\n",
    "\n",
    "关键要点\n",
    "\n",
    "构建 tokenizer（自写 BPE 或用简单词表）\n",
    "\n",
    "padding、batching\n",
    "\n",
    "生成 mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed393f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        return self.proj(x)  # (batch, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "592abb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65253ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "seq_len = 10\n",
    "input_dim = 1\n",
    "d_model = 64\n",
    "\n",
    "# 假数据\n",
    "x = torch.randn(batch_size, seq_len, input_dim)\n",
    "#print(x)\n",
    "num_emb = NumericEmbedding(input_dim, d_model)\n",
    "pos_enc = PositionalEncoding(d_model)\n",
    "\n",
    "x_emb = pos_enc(num_emb(x))  # (batch, seq_len, d_model)\n",
    "print(x_emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cb8b5",
   "metadata": {},
   "source": [
    "### Step 2. 构建词嵌入层（Token Embedding + Positional Encoding）\n",
    "功能\n",
    "\n",
    "把 token id 转换为向量，并加上位置信息。\n",
    "\n",
    "输入\n",
    "\n",
    "input_ids：形状 (batch, seq_len)\n",
    "\n",
    "输出\n",
    "\n",
    "x：形状 (batch, seq_len, d_model)\n",
    "\n",
    "关键要点\n",
    "\n",
    "nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "可选 Sinusoidal PE 或 Learnable PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898923c",
   "metadata": {},
   "source": [
    "### Step 3. 构建多头自注意力（Multi-head Self-Attention）\n",
    "功能\n",
    "\n",
    "捕捉序列内部的依赖。\n",
    "\n",
    "输入\n",
    "\n",
    "x：(batch, seq_len, d_model)\n",
    "\n",
    "mask：(batch, seq_len, seq_len)\n",
    "\n",
    "输出\n",
    "\n",
    "attn_out：(batch, seq_len, d_model)\n",
    "\n",
    "关键要点\n",
    "\n",
    "线性映射：Q = xWq, K = xWk, V = xWv\n",
    "\n",
    "按头拆分\n",
    "\n",
    "Attention:\n",
    "\n",
    "softmax(QKᵀ / sqrt(d_k)) V\n",
    "\n",
    "\n",
    "拼接 heads\n",
    "\n",
    "输出层：W_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b47d1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 线性映射 Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 输出层 WO\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, d_model)\n",
    "        return: (batch, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.size()\n",
    "        x = x.view(batch, seq_len, self.num_heads, self.d_k)\n",
    "        return x.permute(0, 2, 1, 3)  # (batch, heads, seq, d_k)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, d_model)\n",
    "        mask: (batch, 1, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # 1. 投影 Q K V\n",
    "        Q = self.split_heads(self.w_q(x))\n",
    "        K = self.split_heads(self.w_k(x))\n",
    "        V = self.split_heads(self.w_v(x))\n",
    "        # Q,K,V: (batch, heads, seq_len, d_k)\n",
    "\n",
    "        # 2. 注意力分数 QK^T / sqrt(d_k)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        # (batch, heads, seq_len, seq_len)\n",
    "\n",
    "        # 3. 加 mask（非常重要）\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 4. softmax 归一化\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # 5. attention 输出：Softmax * V\n",
    "        attn = torch.matmul(attn_weights, V)\n",
    "        # (batch, heads, seq_len, d_k)\n",
    "\n",
    "        # 6. 拼回原来形状\n",
    "        attn = attn.permute(0, 2, 1, 3).contiguous()\n",
    "        batch, seq_len, _, _ = attn.size()\n",
    "        attn = attn.view(batch, seq_len, self.d_model)\n",
    "        # attn: (batch, seq_len, d_model)\n",
    "\n",
    "        # 7. 通过输出线性层\n",
    "        out = self.w_o(attn)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3967803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "seq_len = 5\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "\n",
    "x = torch.randn(batch, seq_len, d_model)\n",
    "\n",
    "# 示例 mask（全1表示不屏蔽）\n",
    "mask = torch.ones(batch, 1, seq_len, seq_len)\n",
    "\n",
    "mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
    "output = mhsa(x, mask)\n",
    "\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417fd19",
   "metadata": {},
   "source": [
    "### Step 4. 残差 + LayerNorm（Post-LN or Pre-LN）\n",
    "功能\n",
    "\n",
    "稳定深度训练。\n",
    "\n",
    "输入\n",
    "\n",
    "x\n",
    "\n",
    "attn_out\n",
    "\n",
    "输出\n",
    "\n",
    "x1 = LayerNorm(x + attn_out)\n",
    "\n",
    "关键要点\n",
    "\n",
    "Transformer 的基本结构单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ed7e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlockPostLN(nn.Module):\n",
    "    \"\"\"\n",
    "    一个完整的 Attention Block（Post-LN 版本）：\n",
    "    输入: x\n",
    "    输出: x1 = LayerNorm(x + MultiHeadSelfAttention(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        from typing import Optional\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, d_model)\n",
    "        mask: (batch, 1, seq_len, seq_len) 或 None\n",
    "        \"\"\"\n",
    "        attn_out = self.attn(x, mask)           # (batch, seq_len, d_model)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        \n",
    "        # 残差 + LayerNorm  (Post-LN)\n",
    "        x1 = self.norm(x + attn_out)\n",
    "        return x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196e95a",
   "metadata": {},
   "source": [
    "### Step 5. 前馈网络（Feed Forward Network, FFN）\n",
    "功能\n",
    "\n",
    "在 token 维度上进行非线性变换。\n",
    "\n",
    "输入\n",
    "\n",
    "x1：(batch, seq_len, d_model)\n",
    "\n",
    "输出\n",
    "\n",
    "x2：(batch, seq_len, d_model)\n",
    "\n",
    "关键要点\n",
    "\n",
    "标准 FFN：\n",
    "\n",
    "FFN = Linear(d_model, d_ff)\n",
    "      → ReLU/GELU\n",
    "      → Linear(d_ff, d_model)\n",
    "\n",
    "### Step 6. 第二次残差 + LayerNorm\n",
    "功能\n",
    "\n",
    "保持稳定与梯度平衡。\n",
    "\n",
    "输入\n",
    "\n",
    "x1\n",
    "\n",
    "ffn_out\n",
    "\n",
    "输出\n",
    "\n",
    "x_out = LayerNorm(x1 + ffn_out)\n",
    "\n",
    "关键要点\n",
    "\n",
    "一个 EncoderLayer ⇢ Attention + FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03d09daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    前馈网络：作用在序列中的每个 token 上（逐位置独立）\n",
    "    FFN = Linear(d_model → d_ff) → GELU → Linear(d_ff → d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)        # 可选 relu 或 gelu，GPT/big models 基本已经标准化为 GELU\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "886dc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNBlockPostLN(nn.Module):\n",
    "    \"\"\"\n",
    "    x2 = LayerNorm(x1 + FFN(x1))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        ffn_out = self.ffn(x1)\n",
    "        x2 = self.norm(x1 + self.dropout(ffn_out))\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46c73df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    一个完整的 Transformer Encoder Layer（推荐 Pre-LN 实现）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_block = SelfAttentionBlockPostLN(d_model, num_heads, dropout)\n",
    "        self.ffn_block   = FFNBlockPostLN(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x1 = self.attn_block(x, mask)   # Step 3 + Step 4\n",
    "        x2 = self.ffn_block(x1)         # Step 5 + Step 6\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30b0f0",
   "metadata": {},
   "source": [
    "### Step 7. 堆叠 N 层 Encoder\n",
    "功能\n",
    "\n",
    "获得深层语义。\n",
    "\n",
    "输入\n",
    "\n",
    "x_emb\n",
    "\n",
    "mask\n",
    "\n",
    "输出\n",
    "\n",
    "encoder_output：(batch, seq_len, d_model)\n",
    "\n",
    "关键要点\n",
    "\n",
    "通常 N = 6 / 12 / 24 / 48\n",
    "\n",
    "每层共享结构但权重不共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaddbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1, final_norm=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        # 有的实现会在最后再加一层 LayerNorm\n",
    "        self.final_norm = nn.LayerNorm(d_model) if final_norm else None\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x:    (batch, seq_len, d_model)  —— 一般是 embedding + pos_encoding 后的结果\n",
    "        mask: (batch, 1, seq_len, seq_len) 或 None\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)   # 每一层保持形状不变 (batch, seq_len, d_model)\n",
    "\n",
    "        if self.final_norm is not None:\n",
    "            x = self.final_norm(x)\n",
    "\n",
    "        # x 就是 encoder_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba0c2398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "seq_len = 10\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 256\n",
    "num_layers = 6\n",
    "\n",
    "x_emb = torch.randn(batch, seq_len, d_model)\n",
    "mask = torch.ones(batch, 1, seq_len, seq_len)  # 这里随便给个全 1 的 mask\n",
    "\n",
    "encoder = Encoder(num_layers, d_model, num_heads, d_ff, dropout=0.1, final_norm=True)\n",
    "encoder_output = encoder(x_emb, mask)\n",
    "\n",
    "print(encoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7d30b",
   "metadata": {},
   "source": [
    "### Step 8. Decoder（可选，用于语言生成模型）\n",
    "\n",
    "如果你只要 Encoder-only（BERT/ViT）可以跳过。\n",
    "\n",
    "8.1 Decoder Self-Attention\n",
    "\n",
    "带 causal mask。\n",
    "\n",
    "8.2 Encoder-Decoder Attention\n",
    "\n",
    "Q 来自 decoder，KV 来自 encoder。\n",
    "\n",
    "8.3 FFN + 残差 + LN\n",
    "输出\n",
    "\n",
    "decoder_output(batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f3732fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Multi-Head Attention（已支持 cross-attention）\n",
    "# ------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    通用 Attention，可用于 Self-Attention 和 Cross-Attention\n",
    "    Q = query\n",
    "    K,V = key,value （可来自 encoder 或 decoder）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch, seq_len, d_model = x.size()\n",
    "        x = x.view(batch, seq_len, self.num_heads, self.d_k)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.w_q(query))\n",
    "        K = self.split_heads(self.w_k(key))\n",
    "        V = self.split_heads(self.w_v(value))\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn = torch.matmul(attn_weights, V)\n",
    "\n",
    "        attn = attn.permute(0, 2, 1, 3).contiguous()\n",
    "        batch, seq_len, _, _ = attn.size()\n",
    "        attn = attn.view(batch, seq_len, -1)\n",
    "\n",
    "        return self.w_o(attn)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# FFN\n",
    "# ------------------------------\n",
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Decoder Layer（包含三部分）\n",
    "# ------------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    decoder layer:\n",
    "    1. masked self-attention\n",
    "    2. encoder-decoder cross-attention\n",
    "    3. FFN\n",
    "    都采用 Pre-LN（更稳定）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ① masked self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # ② encoder-decoder cross attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # ③ FFN\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch, tgt_seq_len, d_model)\n",
    "        encoder_output: (batch, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # ---- 8.1 Decoder masked Self-Attention ----\n",
    "        x_norm = self.norm1(x)\n",
    "        self_attn_out = self.self_attn(x_norm, x_norm, x_norm, mask=self_mask)\n",
    "        x = x + self.dropout1(self_attn_out)\n",
    "\n",
    "        # ---- 8.2 Encoder–Decoder Cross-Attention ----\n",
    "        x_norm = self.norm2(x)\n",
    "        cross_out = self.cross_attn(\n",
    "            x_norm,                # Q 来自 decoder\n",
    "            encoder_output,        # K 来自 encoder\n",
    "            encoder_output,        # V 来自 encoder\n",
    "            mask=cross_mask\n",
    "        )\n",
    "        x = x + self.dropout2(cross_out)\n",
    "\n",
    "        # ---- 8.3 FFN + 残差 + LN ----\n",
    "        x_norm = self.norm3(x)\n",
    "        ffn_out = self.ffn(x_norm)\n",
    "        x = x + self.dropout3(ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Decoder：堆叠 N 个 DecoderLayer\n",
    "# ------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm_final = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, self_mask, cross_mask)\n",
    "        return self.norm_final(x)     # decoder_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3c990",
   "metadata": {},
   "source": [
    "### Step 9. 输出层（LM Head）\n",
    "功能\n",
    "\n",
    "将模型输出转换为 token 概率。\n",
    "\n",
    "输入\n",
    "\n",
    "hidden_states：(batch, seq_len, d_model)\n",
    "\n",
    "输出\n",
    "\n",
    "logits：(batch, seq_len, vocab_size)\n",
    "\n",
    "关键要点\n",
    "\n",
    "Linear(d_model → vocab_size)\n",
    "\n",
    "通常 tied weight：共享 embedding 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf394eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear(d_model -> vocab_size)\n",
    "    可选：权重共享 (tied embeddings)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab_size, embedding_weight=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # 如果共享权重\n",
    "        if embedding_weight is not None:\n",
    "            # embedding_weight: (vocab_size, d_model)\n",
    "            self.fc.weight = embedding_weight  # tied weights\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        hidden_states: (batch, seq_len, d_model)\n",
    "        return logits:  (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        logits = self.fc(hidden_states)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d742316",
   "metadata": {},
   "source": [
    "### Step 10. Loss 计算\n",
    "功能\n",
    "\n",
    "训练目标。\n",
    "\n",
    "输入\n",
    "\n",
    "logits\n",
    "\n",
    "target_ids\n",
    "\n",
    "输出\n",
    "\n",
    "loss\n",
    "\n",
    "关键要点\n",
    "\n",
    "CrossEntropy\n",
    "\n",
    "注意忽略 PAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d50bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Step 10: LM loss with PAD masking\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_id):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "    def forward(self, logits, target_ids):\n",
    "        \"\"\"\n",
    "        logits: (batch, seq_len, vocab_size)\n",
    "        target_ids: (batch, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch, seq_len, vocab_size = logits.size()\n",
    "\n",
    "        # reshape for CE\n",
    "        logits = logits.view(batch * seq_len, vocab_size)\n",
    "        targets = target_ids.view(batch * seq_len)\n",
    "\n",
    "        loss = self.ce(logits, targets)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df49f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1550)\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "seq_len = 5\n",
    "vocab_size = 30000\n",
    "d_model = 64\n",
    "pad_id = 0\n",
    "\n",
    "logits = torch.randn(batch, seq_len, vocab_size)\n",
    "target_ids = torch.tensor([\n",
    "    [12, 93, 201, 1022, pad_id],   # 最后一个位置是 PAD\n",
    "    [84, 1, 0, 11, pad_id]\n",
    "])\n",
    "\n",
    "criterion = LMLoss(pad_id)\n",
    "loss = criterion(logits, target_ids)\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604196d6",
   "metadata": {},
   "source": [
    "### Step 11. 训练循环（Training Loop）\n",
    "功能\n",
    "\n",
    "反向传播并更新权重。\n",
    "\n",
    "输入\n",
    "\n",
    "batch 数据\n",
    "\n",
    "model\n",
    "\n",
    "optimizer\n",
    "\n",
    "输出\n",
    "\n",
    "训练日志、loss 曲线\n",
    "\n",
    "关键要点\n",
    "\n",
    "AdamW optimizer\n",
    "\n",
    "学习率调度（warmup）\n",
    "\n",
    "gradient clipping 防止爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a8b56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    \"\"\"\n",
    "    Linear warmup + cosine decay\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        # linear decay\n",
    "        return max(\n",
    "            0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps))\n",
    "        )\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    grad_clip=1.0,\n",
    "    warmup_steps=500,\n",
    "    total_steps=10000\n",
    "):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 学习率调度器\n",
    "    scheduler = get_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "        for batch in pbar:\n",
    "\n",
    "            # -----------------------\n",
    "            # 1. 加载 batch 数据\n",
    "            # -----------------------\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            mask       = batch.get(\"mask\", None)\n",
    "            if mask is not None:\n",
    "                mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # -----------------------\n",
    "            # 2. 前向传播\n",
    "            # -----------------------\n",
    "            logits = model(input_ids, mask=mask)   # (batch, seq_len, vocab)\n",
    "\n",
    "            # -----------------------\n",
    "            # 3. loss\n",
    "            # -----------------------\n",
    "            loss = criterion(logits, target_ids)\n",
    "\n",
    "            # -----------------------\n",
    "            # 4. 反向传播\n",
    "            # -----------------------\n",
    "            loss.backward()\n",
    "\n",
    "            # -----------------------\n",
    "            # 5. gradient clipping\n",
    "            # -----------------------\n",
    "            if grad_clip is not None:\n",
    "                clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()        # 学习率调整\n",
    "\n",
    "            global_step += 1\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888679a6",
   "metadata": {},
   "source": [
    "### Step 12. 推理（Inference）\n",
    "功能\n",
    "\n",
    "使用 decoder 进行文本生成。\n",
    "\n",
    "输入\n",
    "\n",
    "prompt / encoder 输出\n",
    "\n",
    "采样策略（greedy, top-k, top-p）\n",
    "\n",
    "输出\n",
    "\n",
    "生成文本\n",
    "\n",
    "关键要点\n",
    "\n",
    "auto-regressive 逐 token 生成\n",
    "\n",
    "需要 causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8032e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len, device):\n",
    "    \"\"\"\n",
    "    生成下三角 causal mask: (1, 1, seq_len, seq_len)\n",
    "    1 代表可见，0 代表被屏蔽\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    # 扩展到 (batch, heads, seq_len, seq_len) 的前两维上留给广播\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def top_k_filtering(logits, top_k):\n",
    "    \"\"\"\n",
    "    只保留 top_k 的 logits，其余设为 -inf\n",
    "    logits: (batch, vocab_size)\n",
    "    \"\"\"\n",
    "    if top_k is None or top_k <= 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, top_k, dim=-1)\n",
    "    min_values = values[:, -1].unsqueeze(-1)  # 每个 batch 的第 k 大值\n",
    "    return torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "\n",
    "def top_p_filtering(logits, top_p):\n",
    "    \"\"\"\n",
    "    nucleus sampling (top-p)\n",
    "    logits: (batch, vocab_size)\n",
    "    \"\"\"\n",
    "    if top_p is None or top_p <= 0.0 or top_p >= 1.0:\n",
    "        return logits\n",
    "\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "    probs = F.softmax(sorted_logits, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "\n",
    "    # 将 cumulative_probs > top_p 的位置 mask 掉\n",
    "    cutoff = cumulative_probs > top_p\n",
    "    # 保证至少保留一个 token\n",
    "    cutoff[..., 0] = False\n",
    "\n",
    "    sorted_logits = sorted_logits.masked_fill(cutoff, float('-inf'))\n",
    "\n",
    "    # 重排回原顺序\n",
    "    logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "    logits_filtered.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
    "    return logits_filtered\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer=None,\n",
    "    prompt_ids=None,      # (batch, prompt_len)\n",
    "    max_new_tokens=50,\n",
    "    eos_token_id=None,\n",
    "    pad_token_id=None,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    greedy=False,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    通用自回归生成函数（Decoder-only 模型）\n",
    "    \n",
    "    model 需要接受:\n",
    "        logits = model(input_ids, mask=causal_mask)\n",
    "        输出: (batch, seq_len, vocab_size)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # shape: (batch, cur_len)\n",
    "    input_ids = prompt_ids.to(device)\n",
    "\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        cur_len = input_ids.size(1)\n",
    "\n",
    "        # 1. 构建 causal mask\n",
    "        causal_mask = create_causal_mask(cur_len, device=device)\n",
    "\n",
    "        # 2. 前向传播得到 logits\n",
    "        logits = model(input_ids, mask=causal_mask)  # (batch, cur_len, vocab)\n",
    "        next_token_logits = logits[:, -1, :]         # 只取最后一个位置: (batch, vocab)\n",
    "\n",
    "        # 3. temperature 调整\n",
    "        if temperature is not None and temperature > 0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # 4. 采样策略：greedy / top-k / top-p\n",
    "        if greedy:\n",
    "            # 贪心：直接取 argmax\n",
    "            next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "        else:\n",
    "            # 随机采样：先过滤，再 softmax，再 multinomial\n",
    "            filtered_logits = next_token_logits\n",
    "            if top_k is not None and top_k > 0:\n",
    "                filtered_logits = top_k_filtering(filtered_logits, top_k)\n",
    "            if top_p is not None and top_p < 1.0:\n",
    "                filtered_logits = top_p_filtering(filtered_logits, top_p)\n",
    "\n",
    "            probs = F.softmax(filtered_logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "        # 5. 如果已经结束（生成了 eos），就强行填 pad，不再改变\n",
    "        if eos_token_id is not None:\n",
    "            # 对已经完成的样本，不再生成新 token（用 pad 占位）\n",
    "            next_tokens = torch.where(\n",
    "                finished,\n",
    "                torch.full_like(next_tokens, pad_token_id if pad_token_id is not None else 0),\n",
    "                next_tokens\n",
    "            )\n",
    "            # 更新 finished 状态\n",
    "            finished = finished | (next_tokens == eos_token_id)\n",
    "\n",
    "            # 如果全部 finished，提前停止\n",
    "            if finished.all():\n",
    "                input_ids = torch.cat([input_ids, next_tokens.unsqueeze(-1)], dim=-1)\n",
    "                break\n",
    "\n",
    "        # 6. 拼接到序列末尾，继续循环\n",
    "        input_ids = torch.cat([input_ids, next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # 生成完毕，返回 token 序列\n",
    "    if tokenizer is not None:\n",
    "        # 把每个样本 decode 成文本\n",
    "        texts = [tokenizer.decode(ids.tolist(), skip_special_tokens=True) for ids in input_ids]\n",
    "        return input_ids, texts\n",
    "    else:\n",
    "        return input_ids, None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
